{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "from torch.nn.init import xavier_uniform_, constant_, zeros_, normal_\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#常用函数\n",
    "\n",
    "def weights_init(model):\n",
    "    \"\"\" Initializes the weights of the CNN model using the Xavier\n",
    "    initialization.\n",
    "    \"\"\"\n",
    "    if isinstance(model, nn.Conv2d):\n",
    "        xavier_uniform_(model.weight, gain=math.sqrt(2.0))\n",
    "        constant_(model.bias, 0.1)\n",
    "    elif isinstance(model, nn.BatchNorm2d):\n",
    "        normal_(model.weight, 1.0, 0.02)\n",
    "        zeros_(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet\n",
    "- ALEXNET过程：input:3 channel\n",
    "        input->Conv 11*11s4,96->BN->ReLU->MaxPool 3*3s2->  \n",
    "        Conv 5*5s1,256->BN->ReLU->MaxPool 3*3s2->  \n",
    "        Conv 3*3s1,384->Conv 3*3s1,384->Conv 3*3s1,256->MaxPool 3*3s2->  \n",
    "        FC 4096->FC 4096->FC 1000\n",
    "- 在本实验中，去掉上述过程中的FC层，只使用全卷积网络部分来提取特征\n",
    "- 参考网络结构 https://github.com/rafellerc/Pytorch-SiamFC/blob/master/training/models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineEmbeddingNet(nn.Module):\n",
    "    \"\"\" Definition of the embedding network used in the baseline experiment of\n",
    "    Bertinetto et al in https://arxiv.org/pdf/1704.06036.pdf.\n",
    "    It basically corresponds to the convolutional stage of AlexNet, with some\n",
    "    of its hyperparameters changed.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BaselineEmbeddingNet, self).__init__()\n",
    "        self.fully_conv = nn.Sequential(nn.Conv2d(3, 96, kernel_size=11,\n",
    "                                                  stride=2, bias=True),\n",
    "                                        nn.BatchNorm2d(96),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.MaxPool2d(3, stride=2),\n",
    "\n",
    "                                        nn.Conv2d(96, 256, kernel_size=5,\n",
    "                                                  stride=1, groups=2,\n",
    "                                                  bias=True),\n",
    "                                        nn.BatchNorm2d(256),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.MaxPool2d(3, stride=1),\n",
    "                                        nn.Conv2d(256, 384, kernel_size=3,\n",
    "                                                  stride=1, groups=1,\n",
    "                                                  bias=True),\n",
    "                                        nn.BatchNorm2d(384),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Conv2d(384, 384, kernel_size=3,\n",
    "                                                  stride=1, groups=2,\n",
    "                                                  bias=True),\n",
    "                                        nn.BatchNorm2d(384),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Conv2d(384, 32, kernel_size=3,\n",
    "                                                  stride=1, groups=2,\n",
    "                                                  bias=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fully_conv(x)\n",
    "        return output\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet(nn.Module):\n",
    "    \"\"\" The basic siamese network joining network, that takes the outputs of\n",
    "    two embedding branches and joins them applying a correlation operation.\n",
    "    Should always be used with tensors of the form [B x C x H x W], i.e.\n",
    "    you must always include the batch dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_net, upscale=False, corr_map_size=33, stride=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_net: 指需要使用的全卷积网络(我使用ALEXNET的全卷积部分)\n",
    "            corr_map_size：指最后的sore_map的尺寸\n",
    "        \n",
    "        \"\"\"\n",
    "        super(SiameseNet, self).__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "        self.match_batchnorm = nn.BatchNorm2d(1)\n",
    "\n",
    "        self.upscale = upscale\n",
    "        # TODO calculate automatically the final size and stride from the\n",
    "        # parameters of the branch\n",
    "        self.corr_map_size = corr_map_size\n",
    "        self.stride = stride\n",
    "        # Calculates the upscale size based on the correlation map size and\n",
    "        # the total stride of the network, so as to align the corners of the\n",
    "        # original and the upscaled one, which also aligns the centers.\n",
    "        self.upsc_size = (self.corr_map_size-1)*self.stride + 1\n",
    "        # The upscale_factor is the correspondence between a movement in the output\n",
    "        # feature map and the input images. So if a network has a total stride of 4\n",
    "        # and no deconvolutional or upscaling layers, a single pixel displacement\n",
    "        # in the output corresponds to a 4 pixels displacement in the input\n",
    "        # image. The easiest way to compensate this effect is to do a bilinear\n",
    "        # or bicubic upscaling.\n",
    "        if upscale:\n",
    "            self.upscale_factor = 1\n",
    "        else:\n",
    "            self.upscale_factor = self.stride\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x1 (torch.Tensor): The reference patch of dimensions [B, C, H, W].\n",
    "                Usually the shape is [8, 3, 127, 127].\n",
    "            x2 (torch.Tensor): The search region image of dimensions\n",
    "                [B, C, H', W']. Usually the shape is [8, 3, 255, 255].\n",
    "        Returns:\n",
    "            match_map (torch.Tensor): The score map for the pair. For the usual\n",
    "                input shapes, the output shape is [8, 1, 33, 33].\n",
    "        \"\"\"\n",
    "        embedding_reference = self.embedding_net(x1)\n",
    "        embedding_search = self.embedding_net(x2)\n",
    "        match_map = self.match_corr(embedding_reference, embedding_search)\n",
    "        return match_map\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.embedding_net(x)\n",
    "\n",
    "    def match_corr(self, embed_ref, embed_srch):\n",
    "        \"\"\" Matches the two embeddings using the correlation layer. As per usual\n",
    "        it expects input tensors of the form [B, C, H, W].\n",
    "        Args:\n",
    "            embed_ref: (torch.Tensor) The embedding of the reference image, or\n",
    "                the template of reference (the average of many embeddings for\n",
    "                example).\n",
    "            embed_srch: (torch.Tensor) The embedding of the search image.\n",
    "        Returns:\n",
    "            match_map: (torch.Tensor) The correlation between\n",
    "        \"\"\"\n",
    "        b, c, h, w = embed_srch.shape\n",
    "        # 在这里，相关层是使用F.conv2d函数实现，为了处理batch维度，使用groups参数的trick。\n",
    "        # 简单来说就是把Batch维度concat到channel维度中（使其成为[1 x（B.C）x H'x W']），并将组数设置为batch的大小。\n",
    "        # 尽管不明显，但这种分组的卷积/相关性等效于两个图像之间的相关性。\n",
    "        \n",
    "        # F.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor\n",
    "        # 效果是把weight作为卷积核对input进行卷积操作\n",
    "        # 为什么要进行concat操作？\n",
    "        # input的形状为(batch,in_channels,iH,iW) weight的形状为(out_channels,in_channels/groups,kH,kW) \n",
    "        # 等于有out_channels个数个in_channels/groups通道数的kH*kW卷积核对input进行卷积操作\n",
    "        # 在这里embed_ref形状为(b,c,h/2,w/2),in_channels为b*c，in_channels/groups=c，刚好满足条件。\n",
    "        # 因此函数得到的match_map的形状为(1,b,oH,oW)，这样通道数b就是批次数\n",
    "        match_map = F.conv2d(embed_srch.view(1, b * c, h, w),\n",
    "                             embed_ref, groups=b)\n",
    "        # Here we reorder the dimensions to get back the batch dimension.\n",
    "        # 利用permute函数对维度进行重新排序。(1,b,oH,oW)->(b,1,oH,oW)\n",
    "        match_map = match_map.permute(1, 0, 2, 3)\n",
    "        #正则化映射至[0,1],此处利用的就是BN\n",
    "        match_map = self.match_batchnorm(match_map)\n",
    "        if self.upscale:\n",
    "            match_map = F.interpolate(match_map, self.upsc_size, mode='bilinear',\n",
    "                                      align_corners=False)\n",
    "\n",
    "        return match_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 33, 33])\n"
     ]
    }
   ],
   "source": [
    "# Test ： 查看结果的尺寸是否和希望的torch.Size([8, 1, 33, 33])一致\n",
    "z=torch.randn(8,3,127,127)\n",
    "x=torch.randn(8,3,255,255)\n",
    "base=BaselineEmbeddingNet()\n",
    "net=SiameseNet(base)\n",
    "output=net(z,x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重定义Traker类\n",
    "- 再次重定义是为了使用GOT-10K工具，这样就不需要写dataset类了:)\n",
    "- 同时在类内定义了使用的优化器、box\n",
    "- 参考实现：https://github.com/got-10k/siamfc/blob/master/siamfc.py\n",
    "- init()的理解参考：https://blog.csdn.net/laizi_laizi/article/details/104622760/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from got10k.trackers import Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword argument repeated (<ipython-input-31-2c96e0e8f8b3>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-31-2c96e0e8f8b3>\"\u001b[1;36m, line \u001b[1;32m25\u001b[0m\n\u001b[1;33m    lr=self.cfg.initial_lr,\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m keyword argument repeated\n"
     ]
    }
   ],
   "source": [
    "class TrackerSiamFC(Tracker):\n",
    "    def __init__(self,pretrained_model=None,**kargs):\n",
    "        super(TrackerSiamFC,self).__init__(name='SiamFC',is_deterministic=True)\n",
    "        # parse_args是下面定义的一个函数，目的是自定义参数。\n",
    "        self.cfg=self.parse_args(**kargs)\n",
    "        \n",
    "        # cuda\n",
    "        self.device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # setup pretrained model\n",
    "        # 实例化前面定义的网络\n",
    "        self.base=BaselineEmbeddingNet()\n",
    "        self.net=SiameseNet(base)\n",
    "        if pretrained_model is not None:\n",
    "            # map_location的作用主要是在不同设备上进行转换(原来的device->cpu)\n",
    "            self.net.load_state_dict(torch.load(pretrained_model,map_location=lambda storage, loc: storage))\n",
    "            # 再从cpu->device\n",
    "        self.net=self.net.to(self.devcie)\n",
    "        \n",
    "        \n",
    "        # setup optimizer\n",
    "        self.optimizer=optim.SGD(\n",
    "            self.net.parameters(),\n",
    "            lr=self.cfg.initial_lr,\n",
    "            lr=self.cfg.initial_lr,\n",
    "            weight_decay=self.cfg.weight_decay,\n",
    "            momentum=self.cfg.momentum\n",
    "        )\n",
    "        \n",
    "        # setup lr scheduler\n",
    "        # ExponentialLR是按照gamma指数衰减\n",
    "        self.lr_scheduler=optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=self.cfg.lr_decay)\n",
    "        \n",
    "    def parse_args(self,**kargs):\n",
    "        # 默认的cfg\n",
    "        cfg = {\n",
    "            # inference parameters\n",
    "            'exemplar_sz': 127,\n",
    "            'instance_sz': 255,\n",
    "            'context': 0.5,\n",
    "            'scale_num': 3,\n",
    "            'scale_step': 1.0375,\n",
    "            'scale_lr': 0.59,\n",
    "            'scale_penalty': 0.9745,\n",
    "            'window_influence': 0.176,\n",
    "            'response_sz': 17,\n",
    "            'response_up': 16,\n",
    "            'total_stride': 8,\n",
    "            'adjust_scale': 0.001,\n",
    "            # train parameters\n",
    "            'initial_lr': 0.01,\n",
    "            'lr_decay': 0.8685113737513527,\n",
    "            'weight_decay': 5e-4,\n",
    "            'momentum': 0.9,\n",
    "            'r_pos': 16,\n",
    "            'r_neg': 0}\n",
    "        \n",
    "        for key, val in kargs.items():#取出可变字典的内容\n",
    "            if key in cfg:\n",
    "                cfg.update({key: val})#更新参数\n",
    "        \n",
    "        # return namedtuple的用法：\n",
    "        # 例如x={'a':1,'b':2} k=namedtuple('GenericDict', x.keys())(**x)\n",
    "        # 其结果k是GenericDict(a=1, b=2)\n",
    "        # 此时可以用类似属性值的方式获得值\n",
    "        # 比如k.a就可以得到1\n",
    "        # 因此，此处namedtuple('GenericDict', cfg.keys())(**cfg)\n",
    "        # 是将字典cfg也作为namedtuple，方便self.cfg.x的方式去使用\n",
    "        return namedtuple('GenericDict', cfg.keys())(**cfg)\n",
    "    \n",
    "    def init(self,image,box):\n",
    "        image=np.asarray(image)\n",
    "        \n",
    "        # 输入的是tlwh的box，使用的是center+hw的box，因此需要转化\n",
    "        box = np.array([\n",
    "            box[1] - 1 + (box[3] - 1) / 2,\n",
    "            box[0] - 1 + (box[2] - 1) / 2,\n",
    "            box[3], box[2]], dtype=np.float32)\n",
    "        self.center,self.target_sz=box[:2],box[2:] # 取出点和hw\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
